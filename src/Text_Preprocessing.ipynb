{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590cc874",
   "metadata": {},
   "source": [
    "# This notebook contains the text processing algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f45c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stemming algorithm\n",
    "%run czech_stemmer.py light"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85adddc6-cb13-455b-932a-e6735a915997",
   "metadata": {},
   "source": [
    "**Firstly, we import all necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef39ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from stop_words import get_stop_words\n",
    "import simplemma\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow import data \n",
    "from tensorflow import keras\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import gensim\n",
    "import warnings\n",
    "#ignore all Futurewarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db35a08e-9fd9-4592-8499-7947619b0bdb",
   "metadata": {},
   "source": [
    "## In this section we implement functions related to basic test preprocessing e.g. tokenization, stop words removal etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb999e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a list of words that shall never be lowercased beacause of their ambiguous meaning.\n",
    "'''\n",
    "global untouchable_words\n",
    "untouchable_words = ['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8e1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctation(news_articles: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Removes punctation from all news articles in the given series (one-dimensional ndarray).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            news_articles: Series (one-dimensional ndarray) of news articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of punctation.\n",
    "    '''\n",
    "    all_punctuation = string.punctuation\n",
    "    all_punctuation += 'ʺ-„–‘’“”—✔️©'\n",
    "    return news_articles.str.translate(str.maketrans( all_punctuation,' '*len(all_punctuation)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26212c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_urls(news_articles: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Removes urls from all news articles in the given series (one-dimensional ndarray).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of urls.\n",
    "    '''\n",
    "    return news_articles.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71910946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(news_articles: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Removes numerical characters from all news articles in the given series (one-dimensional ndarray).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of numbers.\n",
    "    '''\n",
    "    return news_articles.str.replace('\\d+', '')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf6039-5c92-4811-a536-de3019667b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(news_articles: pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Removes all unnecessary characters (numerals, punctation, urls) from all news articles in the given series (one-dimensional ndarray).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of all unnecessary characters.\n",
    "    '''\n",
    "    return remove_numbers(remove_punctation(remove_urls(news_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b94c5d-cb73-46fb-9fcd-9e0dad9c1859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_articles(news_artricles:pd.Series, length:int)-> pd.Series:\n",
    "    '''\n",
    "    Truncates articles to desired length.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "    length: int\n",
    "            Maximal allow number of words in articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of all unnecessary characters.\n",
    "    '''\n",
    "    \n",
    "    return news_artricles.apply(lambda row: row[:length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5e920-3c3e-468a-a54b-0d68c25c322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_ngrams(row:str,gram_size:int) ->list:\n",
    "    '''\n",
    "    Lambda function which is being applied to concrete news article.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    row: str\n",
    "            News article in form of string.\n",
    "    gram_size: int\n",
    "            Maximal size of n-gram allowed in tokenization.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Article converted into list of token of maximal size.\n",
    "    '''\n",
    "    tokens = [t.lower() if t not in untouchable_words else t for t in word_tokenize(row)]\n",
    "    #result = list(ngrams(tokens, gram_size))\n",
    "    return [list(x) for idx,x in enumerate(list(ngrams(tokens, gram_size)))]\n",
    "    # for idx,x in enumerate(result):\n",
    "    #     result[idx] = list(x)\n",
    "    # return result\n",
    "\n",
    "def tokenize(news_articles: pd.Series,gram_size: int) -> pd.Series:\n",
    "    '''\n",
    "    Tokenizes all texts into n-grams with maximal size given in parameter using lambda function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "           news_articles: Series (one-dimensional ndarray) of news articles.\n",
    "    gram_size: int\n",
    "            Maximal size of n-gram allowed in tokenization.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles tokenized into n-grams with given size.\n",
    "    '''\n",
    "    return news_articles.apply(lambda row: \" \".join(row)).apply(lambda row:create_ngrams(row,gram_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f092fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(news_articles: pd.Series,language:str ='en') ->pd.Series:\n",
    "    '''\n",
    "    Removes stop words from pre-tokenized news articles in given language. \n",
    "    If the entered language is not supported an exception is raised. Finally in removes empty tokens from articles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of already tokenized news articles.\n",
    "    language: str\n",
    "            Language used in articles. Available options are 'en' or 'cs'.\n",
    "            \n",
    "    Rises\n",
    "    -----\n",
    "    ValueError\n",
    "            If the entered language is not supported.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of articles stripped of stop words.\n",
    "    '''\n",
    "\n",
    "    if language=='en':\n",
    "        stop_words = [word for word in nltk.corpus.stopwords.words('english')]\n",
    "    elif language=='cs':\n",
    "        stop_words = get_stop_words('czech')\n",
    "    else:\n",
    "        raise ValueError(\"You have entered the wrong language!\") \n",
    "\n",
    "    #add empty string to set of stpowords\n",
    "    stop_words.append('')\n",
    "    stop_words = set(stop_words)\n",
    "    \n",
    "    return news_articles.apply(lambda row: [w.lower() if w not in untouchable_words else w for w  in row.split() if w.lower() not in stop_words])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(news_articles: pd.Series,language:str='en', embedding:bool = False) -> pd.Series:\n",
    "    '''\n",
    "    Stems the tokens in the articles by specified language. Uses SnowballStemmer of english language and light stemming for the czech language. \n",
    "    If the entered language is not supported an exception is raised.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of already tokenized news articles.\n",
    "    language: string\n",
    "            Language used in articles. Available options are 'en' or 'cs'.\n",
    "    embedding: bool\n",
    "            Tells us if result data will go into feature extraction model or word embedding. Default value is False.\n",
    "            \n",
    "    Rises\n",
    "    -----\n",
    "    ValueError\n",
    "            If the provided language in not supported.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of stemmed tokenized articles.\n",
    "    '''\n",
    "\n",
    "    if language not in ['en','cs']:\n",
    "         raise ValueError(\"You have entered the wrong language!\") \n",
    "        \n",
    "    if embedding:\n",
    "        if language=='en':\n",
    "            snow = SnowballStemmer(\"english\")\n",
    "            return news_articles.apply(lambda row: [snow.stem(word) for word in row])\n",
    "        elif language=='cs':\n",
    "            %run czech_stemmer.py light\n",
    "            return news_articles.apply(lambda row: [cz_stem(word) for word in row])\n",
    "    \n",
    "    if language=='en':\n",
    "        snow = SnowballStemmer(\"english\")\n",
    "        return news_articles.apply(lambda row: [[snow.stem(word) for word in ngram ] for ngram in row])\n",
    "    elif language=='cs':\n",
    "        return news_articles.apply(lambda row: [[cz_stem(word) for word in ngram ] for ngram in row])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61750da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(news_articles: pd.Series,language:str='en', embedding:bool=False) -> pd.Series:\n",
    "    '''\n",
    "    Lemmatize the tokens in the articles by specified language. Uses simplemma lemmatizator for both languages. \n",
    "    If the entered language is not supported an exception is raised.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of already tokenized news articles.\n",
    "    language: string\n",
    "            Language used in articles. Available options are 'en' or 'cs'.\n",
    "    embedding: bool\n",
    "            Tells us if result data will go into feature extraction model or word embedding. Default value is False.\n",
    "    \n",
    "    Rises\n",
    "    -----\n",
    "    ValueError\n",
    "            If the provided language in not supported.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Series of lemmatized tokenized articles.\n",
    "    \n",
    "    '''\n",
    "    if language not in ['en','cs']:\n",
    "         raise ValueError(\"You have entered the wrong language!\") \n",
    "\n",
    "    if embedding:\n",
    "        return news_articles.apply(lambda row: [simplemma.lemmatize(word,lang=language) for word in row ])\n",
    "    \n",
    "    return news_articles.apply(lambda row: [[simplemma.lemmatize(word,lang=language) for word in ngram ] for ngram in row])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf67cb7-66b8-4461-9724-cd96dd49427f",
   "metadata": {},
   "source": [
    "## In this section we implement functions that performs the feature extraction (bag-of-words and tf-idf) from given preprocessed dataset and related ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454d220-8985-4ada-8800-1c3b1d6dfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def covert_tokens_to_strings(news_articles:pd.Series) -> list:\n",
    "    '''\n",
    "    Convert tokenized articles into list of strings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Tokenized articles.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of strings cumulated from tokenized articles.\n",
    "    '''\n",
    "    return [' '.join(map(str, l)) for l in news_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87d6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_BOW(news_articles: pd.Series,ngram_size_range:tuple=(1,1)) ->tuple[np.ndarray, sk.feature_extraction.text.CountVectorizer]:\n",
    "    '''\n",
    "    Creates bag-of-words feature extraction model from preprocessed articles. \n",
    "    Then it transforms input data into form processable by ML model and fits the vectorizer. The input data must be training data.\n",
    "\n",
    "    Parameters\n",
    "    ------\n",
    "    \n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of preprocessed news articles.\n",
    "    ngram_size_rng: tuple\n",
    "            Range of possible sizes of n-grams.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Document-term matrix of train data in form processable to ML models.\n",
    "    sklearn.feature_extraction.text.CountVectorizer\n",
    "        Trained vectorizer with vocabulary dictionary.\n",
    "    '''  \n",
    "    count_vectorizer = CountVectorizer(lowercase=False,ngram_range=ngram_size_range,max_features=10000)\n",
    "    \n",
    "    X = count_vectorizer.fit_transform(covert_tokens_to_strings(news_articles))\n",
    "    return X.toarray(),count_vectorizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf(news_articles: pd.Series,ngram_range:tuple=(1,1)) -> tuple[np.ndarray, sk.feature_extraction.text.TfidfVectorizer]:\n",
    "   '''\n",
    "    Creates tfidf feature extraction model from preprocessed articles. \n",
    "    Then it transforms input data into a form processable by ML model and fits the vectorizer. The input data must be training data.\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of preprocessed news articles.\n",
    "    ngram_size_rng: tuple\n",
    "            ngram_size_rng: Range of possible sizes of ngrams.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Document-term matrix in form processable to ML models.\n",
    "    sklearn.feature_extraction.text.CountVectorizer\n",
    "        Trained vectorizer with vocabulary dictionary.\n",
    "    '''  \n",
    "   tf_idf_vectorizer = TfidfVectorizer(lowercase=False,ngram_range=ngram_range,max_features=10000)\n",
    "    \n",
    "   X = tf_idf_vectorizer.fit_transform(covert_tokens_to_strings(news_articles))\n",
    "   return X.toarray(),tf_idf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae2ad9-afbc-484a-9ef7-9b430a8f3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vectorizer(news_articles: pd.Series,vectorizer:sk.feature_extraction.text.CountVectorizer|sk.feature_extraction.text.TfidfVectorizer)->np.array:\n",
    "    '''\n",
    "    Applies vectorizer with vocabulary dictionary on test data. Transforms them into suitable form for ML models. \n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    news_articles: pandas.Series\n",
    "            Series (one-dimensional ndarray) of preprocessed news articles - test data.\n",
    "    vectorizer: sk.feature_extraction.text.CountVectorizer or sk.feature_extraction.text.TfidfVectorizer\n",
    "            Pre-trained vectorizer used in this project i.e. tf-idf and bag-of-words.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray \n",
    "        Document-term matrix for test data \n",
    "    '''\n",
    "    return vectorizer.transform(covert_tokens_to_strings(news_articles)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81e01",
   "metadata": {},
   "source": [
    "## In this section we implement functions related to working with word embeddings in both languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641bbe89-a3e2-40a6-9b67-f6d3bc0abb5f",
   "metadata": {},
   "source": [
    "### In this section we prepare functions related to word2vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed76117-fa28-4e91-8b60-1988cb693efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorizer(news_articles:pd.Series,max_tokens:int=10000) -> tf.keras.layers.TextVectorization:\n",
    "    '''\n",
    "    Creates vocabulary of used words from dataset using keras TextVectorization.\n",
    "    \n",
    "    Parameters\n",
    "    ------\n",
    "    news_articles: pd.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "    max_tokens: int\n",
    "            Maximal size of vocabulary of created TextVectorization. Defalut value is set to 10 000.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.layers.TextVectorization\n",
    "        Preprocessing layer that contains necessary vocabulary of the dataset.\n",
    "    '''\n",
    "    vectorizer = TextVectorization(max_tokens=max_tokens+2)\n",
    "    #concat all articles to one string\n",
    "    column_string = [' '.join(map(str, l)) for l in news_articles]\n",
    "    \n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(column_string).batch(128)\n",
    "    \n",
    "    vectorizer.adapt(text_ds)\n",
    "    \n",
    "    return vectorizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d8284-7d28-46a2-9a56-ec1040abe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec(news_articles : pd.Series, embedding_dim:int) -> gensim.models.word2vec:\n",
    "    '''\n",
    "    Creates and pretrains word2vec model on provided data. Number of epochs is set to 20 for better results. \n",
    "    All CPU cores are used during training process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_article: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "\n",
    "    embedding_dim:int\n",
    "            Dimension of embedding vectors specified for given dataset.\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of vectors with the same lenght as input Series. Each vector is computed as mean of all embedding vectors in certain article.  \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    word2vec = Word2Vec(news_articles, vector_size=embedding_dim ,epochs=20,workers=-1)\n",
    "    return word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74957e9c-681d-4515-84b5-30ffa4ee494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_W2V_matrix(w2v_model: gensim.models.word2vec.Word2Vec,vectorizer:tf.keras.layers.TextVectorization,embedding_dimension:int=100) -> np.ndarray:\n",
    "    '''\n",
    "    Create an embedding matrix using embedding trained on train data and vecctorizer using the samed dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w2v_model: gensim.models.word2vec.Word2Vec\n",
    "            Word2Vec embedding model trained on training dataset that contains vectors for all words\n",
    "    vectorizer: tf.keras.layers.TextVectorization\n",
    "            A preprocessing layer which contains vocabulary for concrete dataset.\n",
    "    embedding_dimension: int\n",
    "            Dimension of embedding vectors that corresponds to words. Default value is set to 100, 300 is for czech dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Embedding 2D matrix that is used in embedding layer.\n",
    "    '''\n",
    "    vectorized_data = dict(zip(vectorizer.get_vocabulary(), range(len(vectorizer.get_vocabulary()))))\n",
    "    embedding_matrix = np.zeros((len(vectorizer.get_vocabulary())+2,embedding_dimension))\n",
    "    \n",
    "    for i,tokenized in enumerate(vectorized_data.items()):\n",
    "\n",
    "        if tokenized[0] not in set(w2v_model.wv.index_to_key):\n",
    "            continue\n",
    "        embedding_vector = w2v_model.wv.get_vector(tokenized[0])\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_W2V_layer(train_articles:pd.Series,vectorizer:tf.keras.layers.TextVectorization,input_len:int,embedding_dimension:int=100) ->keras.layers.Embedding:\n",
    "    '''\n",
    "    Creates W2V embedding layer that will be used in neural networks to transform.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_articles: pandas.Series\n",
    "            Is the training part of input dataset based on which the vectors are going to be created.\n",
    "    vectorizer: tensorflow.keras.layers.TextVectorization\n",
    "            Vocabulary of words extracted from training part of the preprocessed dataset.\n",
    "    input_len: int\n",
    "            Maximal length of the article that the whole dataset was truncated to\n",
    "    embedding_dimension: int\n",
    "            Dimension of embedding vectors that corresponds to words. Default value is set to 100, 300 is for czech dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keras.layers.core.embedding.Embedding\n",
    "        Embedding layer that is going to be a part of neural network model. \n",
    "    \n",
    "    '''\n",
    "    w2v_model = create_word2vec(train_articles,embedding_dimension)\n",
    "    embedding_matrix = prepare_W2V_matrix(w2v_model,vectorizer)\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=embedding_dimension,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False)\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76f1e6",
   "metadata": {},
   "source": [
    "### In this section we implement functions that prepares pre-trained GloVe word embeddings for experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a8697",
   "metadata": {},
   "source": [
    "**First, we prepare pretrained GloVe 100D vectors in English from https://github.com/stanfordnlp/GloVe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9da3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "global EN_EMBEDDING_DIM\n",
    "EN_EMBEDDING_DIM = 100\n",
    "EN_GL_EMBEDDING_IDX = {}\n",
    "\n",
    "def create_en_glove_index() -> dict:\n",
    "    '''\n",
    "    Reads pretrained GloVe word embedding from file, creates dictionary of words and their corresponding embedding vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "        Dictionary indexing vectors to concrete words. \n",
    "    '''\n",
    "    glove_embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            glove_embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return glove_embeddings_index\n",
    "EN_GL_EMBEDDING_IDX = create_en_glove_index()\n",
    "%store EN_GL_EMBEDDING_IDX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12fafe-9c5e-4abb-855a-004026b2d75e",
   "metadata": {},
   "source": [
    "**Now, we prepare pretrained Glove 300D vectors in Czech from https://github.com/Svobikl/cz_corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10d6db-c075-4466-aa8d-f148268422b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global CZ_EMBEDDING_DIM\n",
    "\n",
    "CZ_EMBEDDING_DIM = 300\n",
    "CS_GL_EMBEDDING_IDX={}\n",
    "def create_cz_glove_index() -> dict:\n",
    "    '''\n",
    "    Reads pretrained GloVe word embedding from specified file, creates dictionary of words and their corresponding embedding vectors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dictionary\n",
    "        Dictionary indexing vectors to concrete words. \n",
    "    '''\n",
    "    glove_embeddings_index = {}\n",
    "    with open('vectors_cz_glove_dim300_25.txt', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            if re.match(\".*\\s[a-z]\",line):\n",
    "                continue\n",
    "        \n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            if any(c.isdigit()for c in word):\n",
    "                continue\n",
    "            if word.find(\"_\") != -1:\n",
    "                word = word.replace(\"_\", \" \")\n",
    "            \n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            glove_embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return glove_embeddings_index\n",
    "\n",
    "CS_GL_EMBEDDING_IDX = create_cz_glove_index()\n",
    "%store CS_GL_EMBEDDING_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e946dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_glove_matrix(embedding_dict:dict,vectorizer:tf.keras.layers.TextVectorization,embedding_dim:int=100) -> np.ndarray:\n",
    "    '''\n",
    "    Creates an embedding matrix using pretrined embedding and vecctorizer trained on train data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_dict: dictionary\n",
    "            Dictionary of words with corresonding vectors\n",
    "    vectorizer: tensorflow.keras.layers.TextVectorization\n",
    "            A preprocessing layer which contains vocabulary for concrete dataset.\n",
    "    embedding_dim: int\n",
    "            Dimension of embedding vectors that corresponds to words. Default value is set to 100, 300 is for czech dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Embedding 2D matrix that is used in embedding layer\n",
    "    '''\n",
    "    \n",
    "    vectorized_data = dict(zip(vectorizer.get_vocabulary(), range(len(vectorizer.get_vocabulary()))))\n",
    "    embedding_matrix = np.zeros((len(vectorizer.get_vocabulary()),embedding_dim))\n",
    "    for i, tokenized in enumerate(vectorized_data.items()):\n",
    "        embedding_vector = embedding_dict.get(tokenized[0])\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def create_GloVeWE_layer(vectorizer:tf.keras.layers.TextVectorization,embedding_dict:dict,input_length:int, embedding_dim:int=100) ->keras.layers.Embedding:\n",
    "\n",
    "    '''\n",
    "    Creates embedding layer based on pretrained GloVe embedding obtained from existing file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vectorizer: tensorflow.keras.layers.TextVectorization\n",
    "            Vocabulary of words extracted from training part of the preprocessed dataset.\n",
    "    embedding_dict: dictionary\n",
    "            Dictionary where every word from pretrained embedding has corresponing vector.\n",
    "    input_len: int\n",
    "            Maximal length of the article that the whole dataset was truncated to.\n",
    "    embedding_dim: int\n",
    "            Dimension of embedding vectors that corresponds to words. Default value is set to 100, 300 is for czech dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    keras.layers.core.embedding.Embedding\n",
    "        Embedding layer that is going to be a part of neural network model. \n",
    "    '''\n",
    "\n",
    "    \n",
    "    embedding_matrix = prepare_glove_matrix(embedding_dict,vectorizer,embedding_dim)\n",
    "    \n",
    "    embedding_layer = Embedding(\n",
    "        len(vectorizer.get_vocabulary()),\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "        trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa28288-4299-4257-b9b4-886c2646822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_articles(news_articles:pd.Series,vectorizer:tf.keras.layers.TextVectorization) -> np.array:\n",
    "    '''\n",
    "    Convert given dataset in form suitable for vectorizer and apply already-made vectorizer - converts words to corresponding number.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_articles: pandas.Series\n",
    "            Subset of data that is going to be vectorized.\n",
    "    vectorizer: tensorflow.keras.layers.TextVectorization\n",
    "            Vocabulary of words extracted from training part of the preprocessed dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Articles with words converted to corresponding numbers.\n",
    "    '''\n",
    "    news_articles = news_articles.apply(lambda row: ' '.join(row))\n",
    "    return np.array(vectorizer(np.array([[s] for s in news_articles])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98efdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector_text(news_article:list, word_vector_list:gensim.models.keyedvectors.KeyedVectors|dict,embedding_dim:int) -> np.ndarray:\n",
    "\n",
    "    '''\n",
    "    Computes mean of embedding vectors for given article from dataset. It is a help function for the following one.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    article: list\n",
    "            news_article: Single news_article.\n",
    "    word_vector_list: gensim.models.keyedvectors.KeyedVectors or dict\n",
    "            Dictionary that stores embedding vectors for corresponing words\n",
    "    embedding_dim:int\n",
    "        Dimension of embedding vectors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Vector of means of all embedding vectors that corresponds with words in given article.\n",
    "     \n",
    "    '''\n",
    "    \n",
    "    article_vc_representation = np.zeros([len(news_article), embedding_dim])\n",
    "\n",
    "    for i,w in enumerate(news_article):\n",
    "        if w in word_vector_list:\n",
    "            #did not load correctly\n",
    "            if word_vector_list[w].shape == (0,):\n",
    "                continue\n",
    "            article_vc_representation[i] = word_vector_list[w]\n",
    "    return np.mean(article_vc_representation,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def transform_to_vec(news_articles:pd.Series,word_vector_list:gensim.models.keyedvectors.KeyedVectors|dict, embedding_dim:int) -> list:\n",
    "\n",
    "    '''\n",
    "    Transforms series of news_articles into form suitable for ML models by means of their embedding vectors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    news_article: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "    word_vector_list: gensim.models.keyedvectors.KeyedVectors or dict\n",
    "            Dictionary that stores embedding vectors for corresponing words.\n",
    "    embedding_dim: int\n",
    "            Dimension of embedding vectors of given dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of vectors with the same lenght as input Series. Each vector is computed as mean of all embedding vectors in certain article.  \n",
    "    '''\n",
    "    return [mean_vector_text(article, word_vector_list, embedding_dim) for article in news_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486ac19-ac0a-4b5c-99e8-29ab42449294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idenity_function(news_articles: pd.Series,embedding_dim:int)->tuple[pd.Series,int]:\n",
    "    '''\n",
    "    Dummy function that simplify distinguishing between using pretrained word-embedding and trained one.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    news_article: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "    embedding_dim:int\n",
    "            Dimension of embedding vectors of given dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        News articles without any change.\n",
    "    int\n",
    "        Zero for the same output as in other function.\n",
    "    '''\n",
    "    return news_articles, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2b148-7b00-4571-8482-bc57e149a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idenity_function_text_tn(news_articles: pd.Series,lang:str,embedding:bool)->pd.Series:\n",
    "    '''\n",
    "     Dummy function that simplify using word-embedding.\n",
    "     \n",
    "     Parameters\n",
    "    ----------\n",
    "    news_article: pandas.Series\n",
    "            Series (one-dimensional ndarray) of news articles.\n",
    "    embedding_dim:int\n",
    "            Dimension of embedding vectors of given dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        News articles without any change.\n",
    "    '''\n",
    "    return news_articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
