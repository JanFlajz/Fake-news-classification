{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d91896e",
   "metadata": {},
   "source": [
    "# This notebook holds classes of that implements the classification models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be7e6b-4cf9-4d17-bed4-ebcc5066eb3f",
   "metadata": {},
   "source": [
    "This notebook contains these classes:\n",
    "\n",
    "**CModel** is a parent class of machine learning models. Defines the interface for training and prediction.\n",
    "* **CRandom_Forest** child class that implements the random forest classifier  and its parameters..\n",
    "* **CGausian_Naive_Bayes** implements the Naive Bayes classifier.\n",
    "  \n",
    "**CNeuralNetwork** is a parent class of neural network classifiers. Defines the interface for training and prediction.\n",
    "\n",
    "* **CLSTM** implements the LSTM neural network and its parameters.\n",
    "* **CCCN** implements the Convolutional neural network and its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089a5b3-2b9e-4c17-9b9c-e792e8dfbcfc",
   "metadata": {},
   "source": [
    "**Firstly, we import all necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Activation, Dense,Flatten, LSTM, Dropout \n",
    "from keras.layers import Input, InputLayer\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import keras.callbacks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CModel:\n",
    "\n",
    "    '''\n",
    "    This superclass defines interface for the machine learning models classe used in experiments.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "           Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    param_comb: sklearn.model_selection.ParameterGrid\n",
    "                Hyperparameters that are being tuned during training.\n",
    "    best_hyper_params: sklearn.model_selection.ParameterGrid\n",
    "                       Set of hyperparameters for the given model that reached the highest score during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    abstractmethod tune_hyperparameters(): \n",
    "        Abstract method that finds the best model for prediction on provided data using hyperparameter tuning. Then saves the best model \n",
    "        Returns: None\n",
    "    \n",
    "    abstractmethod make_prediction():\n",
    "        Abstract method that does prediciton using best pretrained classifiaction model on provided test data.\n",
    "        Returns: numpy.array\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,Xtrain:list,Xtest:list,ytrain:pd.Series) -> None:\n",
    "\n",
    "        '''\n",
    "        Constructor of superclass. Stores input data to internal parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "               Preprocessed test input samples of articles.\n",
    "        \n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        self.Xtrain = Xtrain\n",
    "        self.Xtest = Xtest\n",
    "        self.ytrain = ytrain\n",
    "        \n",
    "        def tune_hyperparameters(self)->None:\n",
    "            '''\n",
    "            Abstract method that trains given ML model and create a new one with set hyperparameters based on accuracy of k-fold validation.\n",
    "            Returns\n",
    "            -------\n",
    "            None\n",
    "            '''\n",
    "            pass\n",
    "        def make_prediction(self) -> np.ndarray:\n",
    "            '''\n",
    "            Abstract method that takes the hyperparameters based on training. Pretrains it and make prediction on test data. \n",
    "            Retruns prediction result. \n",
    "            Returns\n",
    "            -------\n",
    "            numpy.array\n",
    "                    Calculated prediction on test data.\n",
    "            '''\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRandom_Forest(CModel):\n",
    "    '''\n",
    "    This class holds the random forest ML model. Its a child class of CModel.\n",
    "\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "           Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    param_comb: sklearn.model_selection.ParameterGrid\n",
    "                Hyperparameters that are being tuned during training.\n",
    "    best_hyper_params: sklearn.model_selection.ParameterGrid\n",
    "                       Set of hyperparameters for the given model that reached the highest score during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    tune_hyperparameters(): \n",
    "        Finds the best model for prediction on provided data using hyperparameter tuning. Then saves the best model \n",
    "        Returns None\n",
    "    \n",
    "    make_prediction():\n",
    "        Does prediciton using best pretrained classifiaction model on provided test data.\n",
    "        Returns numpy.array\n",
    "    \n",
    "   \n",
    "    '''\n",
    "    def __init__(self,*args)-> None:\n",
    "        '''\n",
    "        Subclass constructors of Random forest model, reqiures same arguments as constructor of class CModel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "                Preprocessed test input samples of articles.\n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        super(CRandom_Forest,self).__init__(*args)\n",
    "      \n",
    "        self.param_comb = ParameterGrid({'n_estimators' : range(150,200,10), 'max_depth' : range(5,8),\n",
    "                         'bootstrap' : [True, False]})\n",
    "       \n",
    "        \n",
    "        \n",
    "    def tune_hyperparameters(self) -> None:\n",
    "        \n",
    "        '''\n",
    "        Trains random forest model with all possible hyperparameters defined in param_comb and evaluates it. \n",
    "        Finally, it saves the combintion that reached best accuracy in k-fold validation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        val_acc =[]\n",
    "        for params in self.param_comb:\n",
    "            clf_forest = RandomForestClassifier(n_estimators=params['n_estimators'], \n",
    "                 max_depth=params['max_depth'],bootstrap = params['bootstrap'],n_jobs=-1 )\n",
    "            clf_forest.fit(self.Xtrain, self.ytrain)\n",
    "            val_acc.append(np.mean(cross_val_score(clf_forest, self.Xtrain, self.ytrain, cv=5, scoring='balanced_accuracy', n_jobs=-1)))\n",
    "        #plot_performance(val_acc)\n",
    "        self.best_hyper_params = self.param_comb[np.argmax(val_acc)]\n",
    "        \n",
    "\n",
    "\n",
    "    def make_prediciton(self) -> np.array:\n",
    "        '''\n",
    "        Creates new random forest model with set of best hyperparameters, trains it \n",
    "        on train data and calculates prediction for the test set. Finally returns the results of prediction.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Calculated prediction on test data.\n",
    "        '''\n",
    "        self.tune_hyperparameters()\n",
    "\n",
    "        best_forest = RandomForestClassifier(n_estimators=self.best_hyper_params['n_estimators'], \n",
    "                  max_depth=self.best_hyper_params['max_depth'],bootstrap = self.best_hyper_params['bootstrap'],n_jobs=-1  )\n",
    "        best_forest.fit(self.Xtrain,self.ytrain)\n",
    "        return best_forest.predict(self.Xtest)\n",
    "        \n",
    "       \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c293e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGausian_Naive_Bayes(CModel):\n",
    "    '''\n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "           Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    param_comb: sklearn.model_selection.ParameterGrid\n",
    "                Hyperparameters that are being tuned during training.\n",
    "    best_hyper_params: sklearn.model_selection.ParameterGrid\n",
    "                       Set of hyperparameters for the given model that reached the highest score during training.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    tune_hyperparameters(): \n",
    "        Abstract method that finds the best model for prediction on provided data using hyperparameter tuning. Then saves the best model \n",
    "        Returns None\n",
    "    \n",
    "    make_prediction():\n",
    "        Abstract method that does prediciton using best pretrained classifiaction model on provided test data.\n",
    "        Returns numpy.array\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,*args)-> None:\n",
    "        '''\n",
    "        Subclass constructors of Naive bayes model, reqiures same arguments as constructor of class CModel.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "                Preprocessed test input samples of articles.\n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        super(CGausian_Naive_Bayes,self).__init__(*args)\n",
    "        \n",
    "        self.param_comb = ParameterGrid({ 'var_smoothing':np.logspace(-12, -6, num=6, base=10)})\n",
    "        \n",
    "    def tune_hyperparameters(self)-> None:\n",
    "        '''\n",
    "        Trains Gausian Naive Bayes model with all possible hyperparameters defined in param_comb and evaluates it.\n",
    "        Finally, it saves the combintion that reached best accuracy in k-fold validation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        val_acc = []\n",
    "        \n",
    "        for params in self.param_comb:\n",
    "            clf_GaussianNB = GaussianNB(var_smoothing=params['var_smoothing'] )\n",
    "            clf_GaussianNB.fit(self.Xtrain, self.ytrain)\n",
    "            val_acc.append(np.mean(cross_val_score(clf_GaussianNB, self.Xtrain, self.ytrain, cv=5, scoring='balanced_accuracy',n_jobs=-1)))\n",
    "        #plot_performance(val_acc)\n",
    "        self.best_hyper_params = self.param_comb[np.argmax(val_acc)]\n",
    "    def make_prediciton(self)-> np.array:\n",
    "        '''\n",
    "        Creates new gausian naive bayes model with set of best hyperparameters, trains it on train data and calculates prediction for the test set. \n",
    "        Finally returns the results.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Calculated prediction on test data.\n",
    "        '''\n",
    "\n",
    "        self.tune_hyperparameters()\n",
    "\n",
    "        best_nb = GaussianNB(var_smoothing=self.best_hyper_params['var_smoothing'])\n",
    "        best_nb.fit(self.Xtrain,self.ytrain)\n",
    "        return best_nb.predict(self.Xtest)\n",
    "\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNeuralNetwork:\n",
    "    '''\n",
    "    This superclass defines interface for the neural network model classes used in experiments.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "            Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    embedding_layer: keras.layers.Embedding\n",
    "            Pre-made embedding layer in preprocessing.\n",
    "    input_length: int\n",
    "            Maximal size of one article.\n",
    "    preprocessing: string\n",
    "            Current preprocessing, used in the experiment.\n",
    "    log_path: string\n",
    "            File into which are written log data including resaults and chosen hyperparameters.\n",
    "    dataset_path: string\n",
    "            Relative path to folder which stores the results.\n",
    "    labels_count: int\n",
    "            Number of neurons in output layer based on dataset.\n",
    "    loss: string\n",
    "            Defines loss function used in model training.\n",
    "    name: string\n",
    "            Name of neural network model\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    plot_loss(): Plots training and validation loss over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_accuracy(): Plots training and validation accuracy over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_performance(): Prepares best chosen hyperparameters as string output and calls plot_loss and plot_accuracy.\n",
    "    Returns: None\n",
    "    \n",
    "    train_network(): Tunes the hyperparameters of the network using the k-fold validation. Selects the best hyperparameter combination and saves it.\n",
    "    Returns: None\n",
    "    \n",
    "    make_prediciton(): Firstly, it trains the model using train_network function. Then trains the model with the best set of hypeparameters. And returns the prediction on test data.\n",
    "    Returns: numpy.array\n",
    "    \n",
    "    create_model(parameters: ParameterGrid): Abstract method that in each subclasses builds a model with predefined hyperparameters. Returns the architecture of concrete model.\n",
    "    Returns: keras.Sequential         \n",
    "    '''\n",
    "    def __init__(self,Xtrain:np.array,Xtest:np.array,ytrain:np.array, embedding_layer:keras.layers.Embedding, input_length: int,preprocessing:str, log_path:str,dataset_path:str)-> None:\n",
    "        '''\n",
    "        Constructor of superclass. Stores input data to internal parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "                Preprocessed test input samples of articles.\n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "        embedding_layer: keras.layers.Embedding\n",
    "                Pre-made embedding layer in preprocessing.\n",
    "        input_length: int\n",
    "                Maximal size of one article.\n",
    "        preprocessing: string\n",
    "                Current preprocessing, used in the experiment.\n",
    "        log_path: string\n",
    "                File into which are written log data including resaults and chosen hyperparameters.\n",
    "        dataset_path: string\n",
    "                Relative path to folder which stores the results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        self.Xtrain = Xtrain\n",
    "        self.Xtest = Xtest\n",
    "        self.ytrain = ytrain\n",
    "        self.preprocessing = preprocessing\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.labels_count = 1\n",
    "        self.loss = 'binary_crossentropy'\n",
    "        self.input_length = input_length\n",
    "        self.log_path = log_path\n",
    "        self.dataset_path = dataset_path \n",
    "        self.name = ''\n",
    "        self.num_epochs = tf.constant(15)\n",
    "        self.batch_size = tf.constant(32)\n",
    "        \n",
    "        \n",
    "\n",
    "    def plot_loss(self,history:tf.keras.callbacks.History,params: ParameterGrid)-> None:\n",
    "        '''\n",
    "        Plots loss values reached over training.\n",
    "        Parameters\n",
    "        ----------\n",
    "        history: tensorflow.keras.callbacks.History\n",
    "                Training history of the model.\n",
    "        params: ParameterGrid\n",
    "                Chosen hyperparameters that will be saved to log.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        epochs = range(1,self.num_epochs+1)\n",
    "\n",
    "\n",
    "        cm_path = self.dataset_path + 'figures/' + self.name+ ' '+ self.preprocessing+ ' ' + params +'loss.jpg'  \n",
    "        plt.plot(epochs,loss,'b', label = 'Training loss')\n",
    "        plt.plot(epochs,val_loss,'orange', label = 'Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(cm_path)\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def plot_accuracy(self,history:tf.keras.callbacks.History,params: ParameterGrid) -> None:\n",
    "\n",
    "        '''\n",
    "        Plots the accuracy values reached over training.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        history: tensorflow.keras.callbacks.History\n",
    "                Training history of the model.\n",
    "        params: ParameterGrid\n",
    "                Chosen hyperparameters that will be saved to log.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        cm_path = self.dataset_path  + 'figures/'+self.name+ '_'+ self.preprocessing+ '_' + params +'accuracy.jpg'  \n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        epochs = range(1,self.num_epochs+1)\n",
    "        plt.plot(epochs,acc,'b', label = 'Training acc')\n",
    "        plt.plot(epochs,val_acc,'orange', label = 'Validation acc')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.savefig(cm_path)\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n",
    "    \n",
    "    def plot_performance(self,history:tf.keras.callbacks.History,params:ParameterGrid) ->None:\n",
    "        '''\n",
    "        Draws performance of accuracy and loss during training. Finally it writes the loss and the accuarcy to the log file.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        history: tensorflow.keras.callbacks.History\n",
    "                Training history of the model.\n",
    "        params: ParameterGrid\n",
    "                Chosen hyperparameters that will be saved to log.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        params = str(params)\n",
    "        params = params.replace('\\'','').replace('{','').replace('}','').replace(',','').replace(':','').replace(' ','_')\n",
    "        \n",
    "        self.plot_accuracy(history,params)\n",
    "        self.plot_loss(history,params)\n",
    "        \n",
    "\n",
    "    def train_network(self)->None:\n",
    "        '''\n",
    "        Tunes the hyperparameters of the network using the k-fold validation. Selects the best hyperparameter combination and saves it.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        #set to 5\n",
    "        kfold = KFold(n_splits=5, shuffle=True)\n",
    "        val_acc = []\n",
    "        val_loss = []\n",
    "        \n",
    "        for params in self.param_comb:\n",
    "            acc_per_fold = []\n",
    "            loss_per_fold = []\n",
    "            for train, val in kfold.split(self.Xtrain, self.ytrain):\n",
    "                \n",
    "                model = self.create_model(params)\n",
    "                model.compile(loss=self.loss, optimizer=\"adam\", metrics=[\"acc\"])\n",
    "                history = model.fit(self.Xtrain[train], np.array(self.ytrain)[train],validation_data=(self.Xtrain[val], np.array(self.ytrain)[val]), batch_size=self.batch_size, epochs=self.num_epochs,verbose=0)\n",
    "                acc_per_fold.append(history.history['val_acc'])\n",
    "                loss_per_fold.append(history.history['val_loss'])\n",
    "                \n",
    "            self.plot_performance(history,params)\n",
    "        \n",
    "            \n",
    "            val_acc.append(np.mean(acc_per_fold))  \n",
    "            val_loss.append(np.mean(loss_per_fold))\n",
    "        \n",
    "        self.best_hyper_params = self.param_comb[np.argmax(val_acc)] \n",
    "        with open(self.log_path,'a') as f:\n",
    "            f.write(str(self.best_hyper_params))\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "        \n",
    "        return acc_per_fold,loss_per_fold\n",
    "    \n",
    "    def make_prediciton(self)->np.array:\n",
    "        '''\n",
    "        Firstly, it trains the model using train_network function. \n",
    "        Then trains the model with the best set of hypeparameters. And returns the prediction on test data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Calculated prediction on test data.\n",
    "        '''\n",
    "\n",
    "        \n",
    "        self.train_network()\n",
    "        model = self.create_model(self.best_hyper_params)\n",
    "        model.compile(loss=self.loss, optimizer=\"adam\", metrics=[\"acc\"])\n",
    "        model.fit(self.Xtrain, np.array(self.ytrain), batch_size=self.batch_size, epochs=self.num_epochs,verbose=0)\n",
    "        \n",
    "        return np.round(model.predict(self.Xtest,batch_size=self.batch_size,verbose=0))\n",
    "\n",
    "    def create_model(self,parameters: ParameterGrid)->keras.Sequential:\n",
    "        '''\n",
    "        Abstract method that in each subclasses builds a model with predefined hyperparameters. Returns the architecture of concrete model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters: ParameterGrid\n",
    "                New hyperparameters of the model.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        keras.Sequential\n",
    "                NN model with set hyperparameters.\n",
    "        '''\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLSTM(CNeuralNetwork):\n",
    "    '''\n",
    "    This class holds simple LSTM model with embedding layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "            Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    embedding_layer: keras.layers.Embedding\n",
    "            Pre-made embedding layer in preprocessing.\n",
    "    input_length: int\n",
    "            Maximal size of one article.\n",
    "    preprocessing: string\n",
    "            Current preprocessing, used in the experiment.\n",
    "    log_path: string\n",
    "            File into which are written log data including resaults and chosen hyperparameters.\n",
    "    dataset_path: string\n",
    "            Relative path to folder which stores the results.\n",
    "    labels_count: int\n",
    "            Number of neurons in output layer based on dataset.\n",
    "    loss: string\n",
    "            Defines loss function used in model training.\n",
    "    name: string\n",
    "            Name of neural network model\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    plot_loss(): Plots training and validation loss over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_accuracy(): Plots training and validation accuracy over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_performance(): Prepares best chosen hyperparameters as string output and calls plot_loss and plot_accuracy.\n",
    "    Returns: None\n",
    "    \n",
    "    train_network(): Tunes the hyperparameters of the network using the k-fold validation. Selects the best hyperparameter combination and saves it.\n",
    "    Returns: None\n",
    "    \n",
    "    make_prediciton(): Firstly, it trains the model using train_network function. Then trains the model with the best set of hypeparameters. And returns the prediction on test data.\n",
    "    Returns: numpy.array\n",
    "    \n",
    "    create_model(parameters: ParameterGrid): Builds the LSTM model with predetermined hyperparameters. Returns a built one.\n",
    "    Returns: keras.Sequential\n",
    "    '''\n",
    "    def __init__(self,*args)->None:\n",
    "        '''\n",
    "        Subclass constructors of LSTM NN model, reqiures same arguments as constructor of class CModel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "                Preprocessed test input samples of articles.\n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "        embedding_layer: keras.layers.Embedding\n",
    "                Pre-made embedding layer in preprocessing.\n",
    "        input_length: int\n",
    "                Maximal size of one article.\n",
    "        preprocessing: string\n",
    "                Current preprocessing, used in the experiment.\n",
    "        log_path: string\n",
    "                File into which are written log data including resaults and chosen hyperparameters.\n",
    "        dataset_path: string\n",
    "                Relative path to folder which stores the results.\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        super(CLSTM,self).__init__(*args)\n",
    "        self.name = \"LSTM\"\n",
    "        self.param_comb = ParameterGrid({ 'drop_out' : np.arange(0.3,0.6,0.1),'lstm_units': 2 ** np.arange(7,9,1) })\n",
    "        \n",
    "        \n",
    "    def create_model(self,parameters: ParameterGrid)-> keras.Sequential:\n",
    "        #https://www.researchgate.net/publication/354589045_OPCNN-FAKE_Optimized_Convolutional_Neural_Network_for_Fake_News_Detection\n",
    "        '''\n",
    "        Builds the LSTM model with predetermined hyperparameters. Returns a built one.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters: ParameterGrid\n",
    "                New hyperparameters of the model.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        keras.Sequential\n",
    "                NN model with set hyperparameters.\n",
    "        '''\n",
    "        modelLSTM = Sequential()\n",
    "        modelLSTM.add(Input(shape=(self.input_length,)))\n",
    "        modelLSTM.add(self.embedding_layer)\n",
    "        modelLSTM.add(LSTM(int(parameters['lstm_units'])))\n",
    "        modelLSTM.add(Dropout(parameters['drop_out']))\n",
    "        modelLSTM.add(Flatten()) \n",
    "        modelLSTM.add(Dense(self.labels_count,activation='sigmoid'))\n",
    "        return modelLSTM\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9309cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCNN(CNeuralNetwork):\n",
    "    '''\n",
    "    This class holds simple CNN model.\n",
    "    Attributes\n",
    "    ----------\n",
    "    Xtrain: pandas.Series\n",
    "            Preprocessed training input samples of articles. \n",
    "    Xtest: pandas.Series\n",
    "            Preprocessed test input samples of articles.\n",
    "    ytrain: pandas.Series\n",
    "            Training target values used for training the model.\n",
    "    embedding_layer: keras.layers.Embedding\n",
    "            Pre-made embedding layer in preprocessing.\n",
    "    input_length: int\n",
    "            Maximal size of one article.\n",
    "    preprocessing: string\n",
    "            Current preprocessing, used in the experiment.\n",
    "    log_path: string\n",
    "            File into which are written log data including resaults and chosen hyperparameters.\n",
    "    dataset_path: string\n",
    "            Relative path to folder which stores the results.\n",
    "    labels_count: int\n",
    "            Number of neurons in output layer based on dataset.\n",
    "    loss: string\n",
    "            Defines loss function used in model training.\n",
    "    name: string\n",
    "            Name of neural network model\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    plot_loss(): Plots training and validation loss over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_accuracy(): Plots training and validation accuracy over training epochs.\n",
    "    Returns: None\n",
    "    \n",
    "    plot_performance(): Prepares best chosen hyperparameters as string output and calls plot_loss and plot_accuracy.\n",
    "    Returns: None\n",
    "    \n",
    "    train_network(): Tunes the hyperparameters of the network using the k-fold validation. Selects the best hyperparameter combination and saves it.\n",
    "    Returns: None\n",
    "    \n",
    "    make_prediciton(): Firstly, it trains the model using train_network function. Then trains the model with the best set of hypeparameters. And returns the prediction on test data.\n",
    "    Returns: numpy.array\n",
    "    \n",
    "    create_model(parameters: ParameterGrid):  Builds the LSTM model with predetermined hyperparameters. Returns a built one.\n",
    "    Returns: keras.Sequential\n",
    "    '''\n",
    "    def __init__(self,*args) ->None:\n",
    "        '''\n",
    "        Subclass constructors of Covolutional NN model, reqiures same arguments as constructor of class CModel.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        Xtrain: pandas.Series\n",
    "                Preprocessed training input samples of articles. \n",
    "        Xtest: pandas.Series\n",
    "                Preprocessed test input samples of articles.\n",
    "        ytrain: pandas.Series\n",
    "                Training target values used for training the model.\n",
    "        embedding_layer: keras.layers.Embedding\n",
    "                Pre-made embedding layer in preprocessing.\n",
    "        input_length: int\n",
    "                Maximal size of one article.\n",
    "        preprocessing: string\n",
    "                Current preprocessing, used in the experiment.\n",
    "        log_path: string\n",
    "                File into which are written log data including resaults and chosen hyperparameters.\n",
    "        dataset_path: string\n",
    "                Relative path to folder which stores the results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        super(CCNN,self).__init__(*args)\n",
    "        self.name = \"CNN\"\n",
    "        self.param_comb = ParameterGrid({'filters' : 2 ** np.arange(5,8,1), 'drop_out' : np.arange(0.3,0.6,0.1), 'kernel': np.arange(8,14,2) })\n",
    "       \n",
    "    def create_model(self,parameters: ParameterGrid)-> keras.Sequential:\n",
    "        '''\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        parameters: ParameterGrid\n",
    "                New hyperparameters of the model.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        keras.Sequential\n",
    "                NN model with set hyperparameters.\n",
    "        '''\n",
    "        \n",
    "        modelCNN = Sequential()\n",
    "        modelCNN.add(Input(shape=(self.input_length,)))\n",
    "        modelCNN.add(self.embedding_layer)\n",
    "        modelCNN.add(Dropout(parameters['drop_out']))\n",
    "        modelCNN.add(layers.Conv1D(filters=int(parameters['filters']), kernel_size=int(parameters['kernel']), strides=1, padding=\"causal\", activation=\"relu\"))\n",
    "        modelCNN.add(layers.MaxPooling1D(5))\n",
    "        modelCNN.add(Flatten())  \n",
    "        modelCNN.add(layers.Dense(self.labels_count,  activation=\"sigmoid\"))  \n",
    "        return modelCNN\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
