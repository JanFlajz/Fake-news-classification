{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b8e67b-1648-416a-bd18-bf2cb980dd6c",
   "metadata": {},
   "source": [
    "# In this notebook are stored the classes that handle the logic of experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaa6c3-939e-4599-9597-1f5f94240877",
   "metadata": {},
   "source": [
    "This notebook contains these classes:\n",
    "\n",
    "- **CExperimentResult** handles exporting experiment results into tables and figures\n",
    "- **CExperiment** handles the logic of the experiments. Performs text processing and feature extraction and calls the classification models\n",
    "- **CDataset** loads the datasets into a memory and creates instances of the classes above to perform the experiments and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3940dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run ./Models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run ./Text_Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dd521-2796-4cbf-a190-d90f35a11315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define basic constants related to improting the dataset, datasplitting and figures\n",
    "RELIABILITY = 'label'\n",
    "CONTENT = 'article'\n",
    "RD_SEED = 333\n",
    "TEST_RATIO = 0.2\n",
    "CM_LABELS = ['fake news','reliable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6030e-95ee-46a3-b820-e7f5d3c99a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f801e2d-b33d-40b5-be93-373a57202042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CExperimentResult:\n",
    "    '''\n",
    "    The purpose of this class is to save and evaluate results for one experiment per model.\n",
    "\n",
    "    Atributes\n",
    "    --------\n",
    "    dataset_name: string\n",
    "            Name of dataset which is used in the experiment.\n",
    "    model_name: string\n",
    "            Name of machine learning or deep lerning model which is used in concrete experiment.\n",
    "    dataset_path: string\n",
    "            Relative path to all files and folders related to used dataset.\n",
    "    log_path: string\n",
    "            Path to log file in which progress of experiments is save.\n",
    "    results_per_model: dictionary\n",
    "            This dictionary creates one dataframe containing results per model. \n",
    "    current_model: string\n",
    "            Model of which results are being saved.\n",
    "    results_per_model: dictionary\n",
    "            Dictionary that stores results per one model.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    export_results_to_latex(model:str)\n",
    "        Exports final results per model to latex table and saves it to file.\n",
    "        Returns: None\n",
    "    \n",
    "    evaluate_concrete_experiment(ypredictions:list[np.array],ytest:list[np.array], preprocessing:str)\n",
    "        Evaluates results of machine learning experiment per one preprocessing. Computes average confusion matrix and evalutaion metrics. \n",
    "        Finally, saves important results to file and result dataframe.\n",
    "        Returns: None\n",
    "    save_con_matrix()\n",
    "        Adjust computed confusion matrix for exporting and saves it to the dataset folder.\n",
    "        Returns: None\n",
    "    \n",
    "    add_experiment_results()\n",
    "        Test if number of prediciton is the same as split test data. If not, rises ValueError.\n",
    "        Finally passes the data to evaluation.\n",
    "        Returns: None\n",
    "        \n",
    "    '''\n",
    "    \n",
    "\n",
    "    def __init__(self, dataset_name:str,model_name:str,dataset_path:str,log_path:str)->None:\n",
    "        '''\n",
    "        The purpose of this class is to save and evaluate results for one experiment per model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_name: string\n",
    "                Name of dataset which is used in the experiment.\n",
    "        model_name: string\n",
    "                Name of machine learning or deep lerning model which is used in concrete experiment.\n",
    "        dataset_path: string\n",
    "                Relative path to all files and folders related to used dataset.\n",
    "        log_path: string\n",
    "                Path to log file into which the progress of all experiments is saved.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        self.results_per_model = {}\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_path = dataset_path\n",
    "        self.log_path = log_path\n",
    "        self.current_model = model_name\n",
    "        self.results_per_model[model_name] = pd.DataFrame(columns=['Preprocessing','Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "\n",
    "        \n",
    "        \n",
    "    def export_results_to_latex(self, model:str)->None:\n",
    "        '''\n",
    "        Exports results of experiment to latex table and prints some info about experiments to console.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: string\n",
    "                Classification model used for experiments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "\n",
    "        df = self.results_per_model[self.current_model]\n",
    "        latex_table = df.to_latex(index=False,escape=False)\n",
    "        #some minor changes in latex appearance\n",
    "        latex_table = latex_table.replace('\\\\toprule', '')\n",
    "        latex_table = latex_table.replace('\\\\bottomrule', '')\n",
    "        latex_table = latex_table.replace('\\\\midrule', '\\\\midrule \\\\midrule')\n",
    "        latex_table = latex_table.replace('{lllll}', '{l||c|c|c|c}')\n",
    "        \n",
    "        acc_max = df['Accuracy'].astype(float).idxmax()\n",
    "        f1_max = df['F1-Score'].astype(float).idxmax()\n",
    "    \n",
    "        print('Preprocessing with highest accuracy: ' +  str(df.at[acc_max, 'Preprocessing']))\n",
    "        print('Preprocessing with highest f1-score: ' +  str(df.at[f1_max, 'Preprocessing']))\n",
    "        print(\"\")\n",
    "        \n",
    "        with open(self.dataset_path + 'tables/'+ model + '_experiments_results.txt', 'w') as f:\n",
    "            f.write(latex_table)\n",
    "            f.close()\n",
    "        return list(df.loc[acc_max])\n",
    "    \n",
    "    def evaluate_concrete_experiment(self,ypredictions:list[np.array],ytest:list[np.array], preprocessing:str)-> None:\n",
    "        '''\n",
    "        Evaluates results of machine learning experiment per one preprocessing. Computes average confusion matrix and evalutaion metrics. \n",
    "        Finally, saves important results to file and result dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ypredictions: list\n",
    "                Array of predicions computed by classification model per given preprocessing.\n",
    "        ypredictions: list\n",
    "                Array of original test data per given preprocessing.\n",
    "        preprocessing: string\n",
    "                Text that describes used preprocessing in the experiment.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        def save_con_matrix(cm:np.array)->None:\n",
    "            '''\n",
    "            Adjusts computed confusion matrix for exporting and saves it to the dataset folder.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            cm: numpy.array\n",
    "                    Computed confusion matrix.\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            None\n",
    "            '''\n",
    "            \n",
    "            plt.ioff()\n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            \n",
    "            cm_path = self.dataset_path + 'figures/' + self.current_model+ ' '+ preprocessing+'.jpg'\n",
    "            plt.figure(figsize=(8,6))\n",
    "            cm = sns.heatmap(cm, annot=True, cmap='Blues', fmt='.0f',xticklabels=CM_LABELS, yticklabels=CM_LABELS)\n",
    "            cm.set_yticklabels(cm.get_yticklabels(), rotation = 0)\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.ylabel('True label')\n",
    "            plt.savefig(cm_path,)\n",
    "            \n",
    "            plt.cla()\n",
    "            plt.close()\n",
    "            \n",
    "        all_metrics = []\n",
    "        cm = np.zeros((2,2))\n",
    "        for i in range(0,len(ypredictions)):\n",
    "   \n",
    "            accuracy = metrics.accuracy_score(ypredictions[i],ytest[i])\n",
    "            precision =metrics.precision_score(ypredictions[i],ytest[i])\n",
    "            recall = metrics.recall_score(ypredictions[i],ytest[i])\n",
    "            f1score = metrics.f1_score(ypredictions[i],ytest[i])\n",
    "            all_metrics.append([accuracy,precision,recall,f1score])\n",
    "            cm += confusion_matrix(ytest[i], ypredictions[i])\n",
    "        cm = cm/len(ypredictions)\n",
    "        with open(self.log_path, 'a') as f:\n",
    "            f.write(preprocessing + '\\n')\n",
    "            for i in range(0,len(all_metrics)):\n",
    "                for v in all_metrics[i]:\n",
    "                    f.write('%.3f'%v+'\\t')\n",
    "                f.write('\\n')        \n",
    "            \n",
    "            f.write('-------------------------------\\n')\n",
    "            f.close()\n",
    "       \n",
    "        all_metrics = np.mean(all_metrics,axis=0).tolist()\n",
    "        with open(self.log_path, 'a') as f:\n",
    "            for i,v in enumerate(all_metrics):\n",
    "                all_metrics[i] = '%.3f'%v\n",
    "                f.write(all_metrics[i] + '\\t')\n",
    "                \n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "              \n",
    "        all_metrics.insert(0,preprocessing)\n",
    "        self.results_per_model[self.current_model].loc[len(self.results_per_model[self.current_model])] = all_metrics\n",
    "        save_con_matrix(cm)\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "    def add_experiment_results(self,preprocessing:str, ytestdata:list[np.array],ypredictions:list[np.array]) ->None:\n",
    "        '''\n",
    "        Test if number of prediciton is the same as split test data. If not, rises ValueError.\n",
    "        Finally passes the data to evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessing: string\n",
    "                Text that describes used preprocessing in the experiment.\n",
    "        ytest: list\n",
    "                Array of original test data per given preprocessing.\n",
    "        ypredictions: list\n",
    "                Array of predicions computed by classification model per given preprocessing.\n",
    "                \n",
    "        Rises\n",
    "        -----\n",
    "        ValueError\n",
    "            If the number of split of dataset and number of predictions differs.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        '''\n",
    "        \n",
    "        if len(ytestdata) != len(ypredictions):\n",
    "             raise ValueError('Different sizes of test and prediction') \n",
    "        \n",
    "        self.evaluate_concrete_experiment(ypredictions,ytestdata,preprocessing)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501e671-c676-4af7-8670-2c55b2b033c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CExperiment:\n",
    "    '''\n",
    "    The purpose of this class is to perform set of experiments with given dataset. This includes preprocessing functions.\n",
    "    \n",
    "    Atributes\n",
    "    --------\n",
    "    dataset: pandas.DataFrame\n",
    "            Dataset of news articles that is being experimented with.\n",
    "    dataset_name: string\n",
    "            Name of dataset that is being experimented with.        \n",
    "    log_path: string\n",
    "            Path of the log file of concrete dataset.\n",
    "    dataset_folder: string\n",
    "            Relative path to all files and folders related to used dataset.\n",
    "    language: string\n",
    "            Language in which the articles are written.\n",
    "    EMBEDDING_DIM: int\n",
    "            Size of vector from pretrined embedding.\n",
    "    pretrained_embedding: dictionary\n",
    "            Pretrained embedding that contains vectors per word in given language.\n",
    "    preprocessing_function_dict: dictionary\n",
    "            Maps name of preprocessing to its function. Is used in experiments.\n",
    "    ML_model_function_dict: dictionary\n",
    "            Maps name of machine learning model to its function. Is used in experiments.\n",
    "    DL_model_function_dict: dictionary\n",
    "            Maps name of deep learning model to its function. Is used in experiments.\n",
    "    total_experiments: int\n",
    "            The number of times each experiment is be repeated until the results are obtained. Default value is 3.\n",
    "    Methods\n",
    "    -------\n",
    "    split_data(given_dataset:pd.DataFrame)\n",
    "            Splits the given dataset into train and test part. Is called multiple times troughout the experiments.\n",
    "            Returns: list of pandas.Series\n",
    "    refresh_preprocessing( model:str) -> list of strings\n",
    "            Creates list of possible preprocessing for given model.\n",
    "            Returns list of all preprocessing possibilites from given group.\n",
    "    apply_feature_model(Xtrain:pd.Series,Xtest:pd.Series,fm:str,ngram_size:int)\n",
    "             Apply chosen model for feature extraction to train and test set. \n",
    "             Returns: Processed train and test data.\n",
    "    apply_embedding(Xtrain:pd.Series,Xtest:pd.Series,fm:str)\n",
    "            Apply word embedding to train and test set.\n",
    "            Returns: Processed train and test data.\n",
    "    preprocess_DL(Xtrain : pd.Series, Xtest : pd.Series, text_norm : str, fm:str,max_len:int)\n",
    "            Process input data to the form processable by deep learning models.\n",
    "            Returns: Transformed data that are ready for experiments with classification models and embedding layer which is later passed into DL models.\n",
    "    preprocess(self,Xtrain : pd.Series, Xtest : pd.Series, text_norm : str, ngram_size: int, fm:str,max_len,embedding:bool)\n",
    "            Process input data to the form processable by machine learning models.\n",
    "            Returns: Transformed data that are ready for experiments with classification models.\n",
    "    experiment(model:str, experiment_result:CExperimentResult,max_len:int)\n",
    "            Performs machine learning experiments with all possible preprocessing methods.\n",
    "            Returns: Measured average time of an experiment.\n",
    "    \n",
    "    experiment_DL(model:str, experiment_result:CExperimentResult,max_len:int)\n",
    "             Performs all experiments on deeplearning models, measures time of each and save the results.\n",
    "             Returns: Measured average time of an experiment.\n",
    "            \n",
    "    '''\n",
    "\n",
    "    def __init__(self,dataset:pd.DataFrame,log_path:str,dataset_folder:str,dataset_name:str,language:str='en')-> None:\n",
    "        '''\n",
    "        The purpose of this class is to perform set of experiments with given dataset. This includes preprocessing functions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset: pandas.DataFrame\n",
    "            Dataset of news articles that is being experimented with.\n",
    "        dataset_name: string\n",
    "                Name of dataset that is being experimented with.        \n",
    "        log_path: string\n",
    "                Path of the log file of concrete dataset.\n",
    "        dataset_folder: string\n",
    "                Relative path to all files and folders related to used dataset.\n",
    "        language: string\n",
    "                Language in which the articles are written.\n",
    "                \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \n",
    "        '''\n",
    "\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.dataset_name = dataset_name\n",
    "        self.preprocessing_function_dict = {'stemming':stem,'lemmatization':lemmatize,'no':idenity_function_text_tn, 'tokenize':tokenize,\n",
    "                                            'bag-of-words':create_BOW, 'tf–idf':create_tf_idf, 'Word2Vec': create_word2vec,'GloVe':idenity_function}\n",
    "        self.ML_model_function_dict = {'Naive Bayes':CGausian_Naive_Bayes,'Random Forest': CRandom_Forest}\n",
    "        self.DL_model_function_dict = {'LSTM': CLSTM, 'CNN': CCNN}\n",
    "        self.language = language\n",
    "        self.log_path = log_path\n",
    "        self.dataset_folder = dataset_folder\n",
    "        self.EMBEDDING_DIM = 100 if language == 'en' else 300\n",
    "        self.pretrained_embedding = EN_GL_EMBEDDING_IDX if language=='en' else CS_GL_EMBEDDING_IDX\n",
    "        self.total_experiments = 1\n",
    "        \n",
    "\n",
    "    def split_data(self, given_dataset:pd.DataFrame) -> list[pd.Series]:\n",
    "        '''\n",
    "        Splits the given dataset into train and test part. Is called multiple times troughout the experiments.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        given_dataset: pandas.DataFrame\n",
    "                Whole dataset of articles and reliabilities.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of pandas.Series\n",
    "            Train and test data with separated dependent values.\n",
    "        \n",
    "        '''\n",
    "        Xdata = given_dataset[CONTENT]\n",
    "        ydata = given_dataset[RELIABILITY]\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, ydata, test_size=TEST_RATIO, random_state=RD_SEED) \n",
    "        return Xtrain,Xtest,ytrain,ytest\n",
    "\n",
    "\n",
    "    def refresh_preprocessing(self, model:str) -> list[str]:\n",
    "        '''\n",
    "        Creates list of possible preprocessing for given model. Returns list of all preprocessing possibilites from given group.\n",
    "        Paramters\n",
    "        ---------\n",
    "        model: string\n",
    "                Name of current model that is being used for experimenting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of strings\n",
    "            list of possible preprocessing of the given groups.\n",
    "        \n",
    "        '''\n",
    "        text_normalization = ['stemming', 'lemmatization'] \n",
    "        if model not in ['Naive Bayes', 'Random Forest']:\n",
    "            text_normalization.insert(0,'no')\n",
    "        \n",
    "       \n",
    "        ngrams = ['tokenize' for _ in range(3)]\n",
    "            \n",
    "        feature_models = ['bag-of-words', 'tf–idf', 'Word2Vec','GloVe'] if model in ['Naive Bayes', 'Random Forest'] else ['Word2Vec','GloVe']\n",
    "        \n",
    "        return text_normalization,ngrams,feature_models\n",
    "\n",
    "\n",
    "    def apply_feature_model(self,Xtrain:pd.Series,Xtest:pd.Series,fm:str,ngram_size:int) -> list[pd.Series]:\n",
    "        '''\n",
    "        Apply chosen model for feature extraction to train and test set. Returns processed train and test data.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        Xtrain:pd.Series\n",
    "                Preprocessed train data.\n",
    "        Xtest:pd.Series\n",
    "                Preprocessed test data.\n",
    "        fm:str\n",
    "                Feature model chosen for this experiment.\n",
    "        ngram_size:int\n",
    "                Size of n-grams the taxt was split into.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list of pd.Series\n",
    "                Train and test data transformed by feature model.\n",
    "                \n",
    "        '''\n",
    "        \n",
    "        Xtrain,vectorizer = self.preprocessing_function_dict[fm](Xtrain,(ngram_size,ngram_size))\n",
    "        Xtest = apply_vectorizer(Xtest,vectorizer)\n",
    "        \n",
    "        return Xtrain, Xtest\n",
    "    def apply_embedding(self,Xtrain:pd.Series,Xtest:pd.Series,fm:str) ->list[pd.Series]:\n",
    "        '''\n",
    "        Apply word embedding to train and test set. Returns processed train and test data.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        Xtrain:pd.Series\n",
    "                Preprocessed train data.\n",
    "        Xtest:pd.Series\n",
    "                Preprocessed test data.\n",
    "        fm:str\n",
    "                Name of word embedding in this experiment.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        list of pd.Series\n",
    "                Train and test data transformed by word embedding..\n",
    "                \n",
    "        '''\n",
    "    \n",
    "\n",
    "        \n",
    "        emb_vectors = self.preprocessing_function_dict[fm](Xtrain,self.EMBEDDING_DIM)\n",
    "\n",
    "        \n",
    "        emb_vectors = emb_vectors.wv if fm == 'Word2Vec' else self.pretrained_embedding\n",
    "        \n",
    "        Xtrain = transform_to_vec(Xtrain,emb_vectors,self.EMBEDDING_DIM)\n",
    "        Xtest = transform_to_vec(Xtest,emb_vectors,self.EMBEDDING_DIM)\n",
    "        \n",
    "        return Xtrain, Xtest\n",
    "\n",
    "    def preprocess_DL(self,Xtrain : pd.Series, Xtest : pd.Series, text_norm : str, fm:str,max_len:int) -> tuple[list[pd.Series],keras.layers.Embedding]:\n",
    "        '''\n",
    "        Process input data to the form processable by deep learning models.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        Xtrain:pd.Series\n",
    "                Raw train data.\n",
    "        Xtest:pd.Series\n",
    "                Raw test data.\n",
    "        text_norm: str\n",
    "                Name of text normaliztion that is being applied.\n",
    "        fm: str\n",
    "                 Name of feature extraction that is being applied.\n",
    "        max_len:int\n",
    "                Maximal number of words in article after stopwords removal.\n",
    "        Returns\n",
    "        -------\n",
    "        list of pd.Series\n",
    "                Train and test data transformed to experiments.\n",
    "        \n",
    "        keras.layers.Embedding\n",
    "                Embedding layer that is passed to neural network model.\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        Xtrain  = truncate_articles(remove_stop_words(remove_special_characters(Xtrain),self.language),max_len)\n",
    "        Xtest  = truncate_articles(remove_stop_words(remove_special_characters(Xtest),self.language),max_len)\n",
    "        Xtrain = self.preprocessing_function_dict[text_norm](Xtrain,self.language,True) \n",
    "        Xtest = self.preprocessing_function_dict[text_norm](Xtest,self.language,True)\n",
    "        \n",
    "        vectorizer = create_vectorizer(Xtrain)\n",
    "\n",
    "        if fm =='GloVe':\n",
    "            embedding_layer =  create_GloVeWE_layer(vectorizer,self.pretrained_embedding,max_len,self.EMBEDDING_DIM)\n",
    "        else:\n",
    "            embedding_layer =  create_W2V_layer(Xtrain,vectorizer,max_len,self.EMBEDDING_DIM)\n",
    "            \n",
    "        Xtrain = vectorize_articles(Xtrain,vectorizer)\n",
    "        Xtest = vectorize_articles(Xtest,vectorizer)\n",
    "        \n",
    "        #display(Xtrain)   \n",
    "        return Xtrain,Xtest,embedding_layer\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    def preprocess(self,Xtrain : pd.Series, Xtest : pd.Series, text_norm : str, ngram_size: int, fm:str,max_len,embedding:bool=False) -> list[pd.Series]:\n",
    "        '''\n",
    "        Process input data to the form processable by machine learning models.\n",
    "        \n",
    "        Paramters\n",
    "        ---------\n",
    "        Xtrain:pd.Series\n",
    "                Raw train data.\n",
    "        Xtest:pd.Series\n",
    "                Raw test data.\n",
    "        text_norm: str\n",
    "                Name of text normaliztion that is being applied.\n",
    "        ngram_size: int\n",
    "                Size of n-gram in which the text is split.\n",
    "        fm: str\n",
    "                 Name of feature extraction that is being applied.\n",
    "        max_len:int\n",
    "                Maximal number of words in article after stopwords removal.\n",
    "        embedding: bool\n",
    "                Marker that states whether the text is processed with word embedding.\n",
    "        Returns\n",
    "        -------\n",
    "        list of pd.Series\n",
    "                Train and test data transformed to experiments.\n",
    "        '''\n",
    "\n",
    "        \n",
    "      \n",
    "        if embedding:\n",
    "            Xtrain  = truncate_articles(remove_stop_words(remove_special_characters(Xtrain),self.language),max_len)\n",
    "            Xtrain = self.preprocessing_function_dict[text_norm](Xtrain,self.language,True) if fm != 'GloVe' else Xtrain\n",
    "        \n",
    "            Xtest  = truncate_articles(remove_stop_words(remove_special_characters(Xtest),self.language),max_len)\n",
    "            Xtest = self.preprocessing_function_dict[text_norm](Xtest,self.language,True) if fm != 'GloVe' else Xtest\n",
    "            return self.apply_embedding(Xtrain,Xtest,fm)\n",
    "            \n",
    "        Xtrain  = tokenize(truncate_articles(remove_stop_words(remove_special_characters(Xtrain),self.language),max_len),ngram_size)\n",
    "        Xtrain = self.preprocessing_function_dict[text_norm](Xtrain,self.language,False)\n",
    "\n",
    "        Xtest  = tokenize(truncate_articles(remove_stop_words(remove_special_characters(Xtest),self.language),max_len),ngram_size)\n",
    "        Xtest = self.preprocessing_function_dict[text_norm](Xtest,self.language,False)\n",
    "\n",
    "        return self.apply_feature_model(Xtrain,Xtest,fm,ngram_size)\n",
    "   \n",
    "    \n",
    "    def experiment(self,model:str, experiment_result:CExperimentResult,max_len:int) -> None:\n",
    "        '''\n",
    "        Performs machine learning experiments with all possible preprocessing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model:str\n",
    "                Name of classification model that is used for experiment.\n",
    "        experiment_result:CExperimentResult\n",
    "                Object that stores the results of every experiment.\n",
    "        max_len:int\n",
    "                Maximal allowed lenght of article for experiment.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        text_normalization,ngrams,feature_models= self.refresh_preprocessing(model)\n",
    "       \n",
    "        with open(self.log_path, 'a') as f:\n",
    "            f.write(model+'\\n')\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "        total_experiments = 3\n",
    "        time_measuring = []\n",
    "        for tn in text_normalization:\n",
    "            for fm in feature_models:\n",
    "                for ng_size in range(1,4):\n",
    "                    preprocessing_text = tn + ' ' +  str(ng_size) + '-grams' +' ' + fm\n",
    "                    embedding = False\n",
    "                    if fm in ['Word2Vec','GloVe']: \n",
    "                        if ng_size < 3 or fm =='GloVe' and tn != 'lemmatization':\n",
    "                            continue\n",
    "                        embedding = True\n",
    "                        preprocessing_text = tn  +' ' + fm if fm != 'GloVe' else 'pretrained ' + fm\n",
    "                    y_predictions = []\n",
    "                    y_test_data = []\n",
    "                    print(preprocessing_text)\n",
    "                    for i in range(0,self.total_experiments):\n",
    "                        start = time.time()\n",
    "                        Xtrain, Xtest, ytrain, ytest = self.split_data(self.dataset)\n",
    "                        Xtrain, Xtest = self.preprocess(Xtrain,Xtest, tn, ng_size,fm,max_len,embedding)\n",
    "                        ml = self.ML_model_function_dict[model](Xtrain,Xtest,ytrain)                        \n",
    "                        y_test_data.append(ytest.to_numpy())\n",
    "                        y_predictions.append(ml.make_prediciton())\n",
    "                        \n",
    "                        end = time.time()\n",
    "                        time_measuring.append(end-start)\n",
    "                    experiment_result.add_experiment_results(preprocessing_text,y_test_data,y_predictions)\n",
    "                \n",
    "                    \n",
    "        return np.mean(time_measuring)\n",
    "\n",
    "    def experiment_DL(self,model:str, experiment_result:CExperimentResult,max_len:int) -> None:\n",
    "        \n",
    "        '''\n",
    "        Performs all experiments on deeplearning models, measures time of each and save the results.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model:str\n",
    "                Name of classification model that is used for experiment.\n",
    "        experiment_result:CExperimentResult\n",
    "                Object that stores the results of every experiment.\n",
    "        max_len:int\n",
    "                Maximal allowed lenght of article for experiment.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        text_normalization,ngrams,feature_models= self.refresh_preprocessing(model)\n",
    "       \n",
    "        with open(self.log_path, 'a') as f:\n",
    "            f.write(model+'\\n')\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "        embedding = False\n",
    "\n",
    "        total_experiments = 3\n",
    "        time_measuring = []\n",
    "        for fm in feature_models :\n",
    "            for tn in text_normalization:\n",
    "                preprocessing_text = tn  +' ' + fm if tn != 'no' else fm\n",
    "                if fm == 'GloVe':\n",
    "                    if tn != 'no':\n",
    "                        break\n",
    "                    preprocessing_text = 'pretrained ' + fm \n",
    "\n",
    "                y_predictions = []\n",
    "                y_test_data = []\n",
    "                print(preprocessing_text)\n",
    "                \n",
    "                for i in range(0,self.total_experiments):\n",
    "                    start = time.time()\n",
    "                    Xtrain, Xtest, ytrain, ytest = self.split_data(self.dataset)\n",
    "                    \n",
    "                    Xtrain, Xtest, embedding_layer = self.preprocess_DL(Xtrain,Xtest,tn,fm,max_len)\n",
    "                    dl = self.DL_model_function_dict[model](Xtrain, Xtest,ytrain,embedding_layer,max_len,preprocessing_text,self.log_path,self.dataset_folder)\n",
    "                    y_test_data.append(ytest.to_numpy())\n",
    "                    a = y_predictions.append(dl.make_prediciton())\n",
    "                    end = time.time()\n",
    "                    time_measuring.append(end-start)\n",
    "                experiment_result.add_experiment_results(preprocessing_text,y_test_data,y_predictions)\n",
    "                \n",
    "        return np.mean(time_measuring)\n",
    "            \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDataSet:\n",
    "    '''\n",
    "    The purpose of this class is to load dataset for experiments and perform experiments on it.\n",
    "\n",
    "    Atributes\n",
    "    --------\n",
    "    dataset_name: str\n",
    "            The name of chosen dataset.\n",
    "    dataset: pandas.Dataframe  \n",
    "            Data from the chosen dataset.\n",
    "    dataset_path: string\n",
    "            Relative path to csv file that contains dataset data.\n",
    "    output_folder: string\n",
    "            Relative path folder into which we save info about experiments.\n",
    "    log_path: string\n",
    "            Path to log file in which progress of experiments is save.\n",
    "    max_len: int\n",
    "            Maximal number of words that can be used in experiment. Default value is set to 250.\n",
    "    language: string\n",
    "            Language used in articles. Can be only English (en) or Czech (cs).\n",
    "    total_model_results: pandas.DataFrame\n",
    "            This DataFrame saves average results of all metrics per all models.\n",
    "    time_measured: pandas.DataFrame\n",
    "            This DataFrame saves average time spent on experiment per model.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    create_log_file()\n",
    "        Creates log text file for given dataset.\n",
    "        Returns: None\n",
    "    export_final_results_to_latex()\n",
    "        Exports final results to latex table. Finally, saves the graph of the results.\n",
    "        Returns: None\n",
    "    conduct_ML_experiments()\n",
    "        Performs experiments with machine learning models on given dataset. \n",
    "        Writes info about experiment progress to console. Also measures time of all experiments.\n",
    "        Returns: None\n",
    "    conduct_DL_experiment()\n",
    "        Performs experiments with deep learning models on given dataset. Writes info about experiment progress to console. \n",
    "        Also measures time of all experiments.\n",
    "        Returns: None\n",
    "    perform_experiments()\n",
    "        Performs all experiments and saves the results into specified files.\n",
    "        Returns: None\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dataset_name:str, dataset_path:str,output_folder:str,max_len:int=250, language:str = 'en')->None:\n",
    "        '''\n",
    "        The purpose of this class is to load dataset for experiments and perform experiments on it.\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset_name: str\n",
    "            The name of chosen dataset.\n",
    "        dataset_path: string\n",
    "                Relative path to csv file that contains dataset data.\n",
    "        output_folder: string\n",
    "                Relative path folder into which we save info about experiments.\n",
    "        max_len: int\n",
    "                Maximal number of words that can be used in experiment. Default value is set to 250.\n",
    "        language: string\n",
    "                Language used in articles. Can be only English (en) or Czech (cs).\n",
    "                \n",
    "        Rises\n",
    "        -----\n",
    "        ValueError\n",
    "                If specified language is not availibe or max length of dataset is too short.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        if language not in ['en','cs'] or max_len < 0:\n",
    "            raise ValueError('You have entered the wrong language!') \n",
    "        \n",
    "        self.dataset_name=dataset_name\n",
    "        self.dataset_path= dataset_path\n",
    "        self.dataset = pd.read_csv(dataset_path,sep=',',on_bad_lines='skip').dropna()\n",
    "        self.output_folder = output_folder\n",
    "        self.total_model_results = pd.DataFrame(columns=['Model','Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "        self.time_measured = pd.DataFrame(columns = [ '','Naive Bayes', 'Random Forest','CNN', 'LSTM'])\n",
    "\n",
    "        def create_log_file()-> None:\n",
    "            '''\n",
    "            Creates log text file for given dataset.\n",
    "            Returns\n",
    "            -------\n",
    "            None\n",
    "            '''\n",
    "            new_log_path = './' + self.output_folder+'/'+ dataset_name+'_log.txt'\n",
    "            f = open(new_log_path, 'w')\n",
    "            f.write('Created log file for dataset:\\n' + dataset_name+'\\n')\n",
    "            f.close()\n",
    "            self.log_path = new_log_path\n",
    "        create_log_file()\n",
    "        self.language = language\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def export_final_results_to_latex(self)-> None:\n",
    "        '''\n",
    "        Exports final results to latex table. Finally, saves the graph of the results.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        latex_table = self.total_model_results.to_latex(index=False,escape=False)\n",
    "        latex_table = latex_table.replace('{lllll}', '{|l|c|c|c|c|}')\n",
    "        with open(self.output_folder +  'tables/per_model_results.txt', 'w') as f:\n",
    "           f.write(latex_table)\n",
    "           f.close()\n",
    "\n",
    "        self.time_measured.loc[0,''] = 'Avg. time per experiment (s)'\n",
    "        latex_time = self.time_measured.to_latex(index=False,escape=False)\n",
    "        latex_time = latex_time.replace('{lllll}', '{l|c|c|c|c|}')\n",
    "        latex_time = latex_time.replace('\\toprule', '')\n",
    "        latex_time = latex_time.replace('\\bottomrule', '')\n",
    "        with open(self.output_folder +  '/tables/avg_time_results.txt', 'w') as f:\n",
    "           f.write(latex_time)\n",
    "           f.close()\n",
    "        \n",
    "\n",
    "        \n",
    "        for c in ['Accuracy', 'Precision', 'Recall', 'F1-Score']:\n",
    "            self.total_model_results[c]= self.total_model_results[c].astype(float)\n",
    "        plt.ioff()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        self.total_model_results =self.total_model_results.set_index('Model')\n",
    "        self.total_model_results.plot(kind='bar')\n",
    "        plt.legend(loc='upper left',bbox_to_anchor=(1.01, 1))\n",
    "        plt.xlabel('Model')\n",
    "        plt.xticks(rotation=0)\n",
    "        path = self.output_folder + 'figures/'+'all_models_results.jpg'\n",
    "        plt.savefig(path,bbox_inches='tight')\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "    def conduct_ML_experiments(self)-> None:\n",
    "        '''\n",
    "        Performs experiments with machine learning models on given dataset. \n",
    "        Writes info about experiment progress to console. Also measures time of all experiments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        ML_MODELS = [ 'Naive Bayes', 'Random Forest']\n",
    "        print('Performing ML experiments...')\n",
    "        \n",
    "        for m in ML_MODELS:\n",
    "            print(m + '\\n')\n",
    "            new_experiment= CExperiment(self.dataset.copy().sample(frac=1),self.log_path,self.output_folder,self.dataset_name,language=self.language)\n",
    "            experiment_result = CExperimentResult(self.dataset_name,m,self.output_folder,self.log_path)\n",
    "            avg_time = new_experiment.experiment(m,experiment_result, self.max_len)\n",
    "            \n",
    "            self.time_measured.loc[0,m] = str(round(avg_time,2))\n",
    "            \n",
    "            results = experiment_result.export_results_to_latex(m)\n",
    "            results[0] = m\n",
    "            self.total_model_results.loc[len(self.total_model_results)] = results\n",
    "            \n",
    "        \n",
    "    \n",
    "    def conduct_DL_experiments(self) -> None:\n",
    "        '''\n",
    "        Performs experiments with deep learning models on given dataset. Writes info about experiment progress to console. \n",
    "        Also measures time of all experiments.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        \n",
    "        print('Performing DL experiments...')\n",
    "        DL_MODELS = [ 'CNN', 'LSTM']\n",
    "        \n",
    "        for m in DL_MODELS:\n",
    "            print(m + '\\n')\n",
    "            new_experiment= CExperiment(self.dataset.copy().sample(frac=1),self.log_path,self.output_folder,self.dataset_name)\n",
    "            \n",
    "            experiment_result = CExperimentResult(self.dataset_name,m,self.output_folder,self.log_path)\n",
    "            avg_time = new_experiment.experiment_DL(m,experiment_result,self.max_len)\n",
    "            \n",
    "            self.time_measured.loc[0,m] = str(round(avg_time,2))\n",
    "            results = experiment_result.export_results_to_latex(m)\n",
    "            results[0] = m\n",
    "            \n",
    "            self.total_model_results.loc[len(self.total_model_results)] = results\n",
    "            \n",
    "          \n",
    "                   \n",
    "    def perform_experiments(self)-> None:\n",
    "        '''\n",
    "        Performs all experiments and saves the results into specified files.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        self.conduct_ML_experiments()\n",
    "        self.conduct_DL_experiments()\n",
    "        self.export_final_results_to_latex()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
