{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fde84f7-e753-4581-a49c-380464390455",
   "metadata": {},
   "source": [
    "# This notebook serves for performing experiments on all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e3154-5128-4e7e-8d29-b8cdce78aa6e",
   "metadata": {},
   "source": [
    "**The following cell imports all necesary libraries and classes to perform the experiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab51e0f-66d0-45dd-9870-cfa0a4bf0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'EN_GL_EMBEDDING_IDX' (dict)\n",
      "Stored 'CS_GL_EMBEDDING_IDX' (dict)\n"
     ]
    }
   ],
   "source": [
    "run ./Experiments.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758fe67-6104-4e90-889b-f4c8ed4a4c3a",
   "metadata": {},
   "source": [
    "**To perform the experiments on the datasets, run the following cells. This runs the experiments on both machine and deep learning models. To perform experiments on new dataset, create an instance of the class CDataSet with name of the dataset, relative path to the csv file of the data and the output path. Be careful, the csv file must fill the following formula. The text part of the dataset (news articles that are being classified) must be in the column called 'article'. The class of the article must be specified by integer with values 0 and 1 in column called 'label'.**\n",
    "\n",
    "If you want to perform the experiments on just one family of approaches i.e. machine learning or neural networks. Call the following methods on a instance of the dataset:\n",
    "\n",
    "- conduct_ML_experiments() to perform experiments using Naive Bayes and random forest,\n",
    "- conduct_DL_experiments() to perform experiments using Convolutional Neural Network and LSTM netural network,\n",
    "\n",
    "\n",
    "both of these method are parameters-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "428aad30-c296-415f-acac-b5dd7d41a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ML experiments...\n",
      "Naive Bayes\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: lemmatization 2-grams tf–idf\n",
      "Preprocessing with highest f1-score: lemmatization 2-grams tf–idf\n",
      "\n",
      "Random Forest\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: lemmatization 2-grams tf–idf\n",
      "Preprocessing with highest f1-score: lemmatization 2-grams tf–idf\n",
      "\n",
      "Performing DL experiments...\n",
      "CNN\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n",
      "LSTM\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: Word2Vec\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CDataSet('ISOT','./datasets/ISOT/isot.csv', './datasets/ISOT/')\n",
    "dataset.perform_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd286719-bd12-4a3f-ac73-04e2bbcc74e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ML experiments...\n",
      "Naive Bayes\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: stemming 2-grams bag-of-words\n",
      "Preprocessing with highest f1-score: stemming 2-grams bag-of-words\n",
      "\n",
      "Random Forest\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n",
      "Performing DL experiments...\n",
      "CNN\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n",
      "LSTM\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: Word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CDataSet('ReCOVery','./datasets/Recovery/recovery.csv', './datasets/Recovery/')\n",
    "dataset.perform_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2d58eb-a19d-48b2-bf4a-b3697eb947ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ML experiments...\n",
      "Naive Bayes\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: lemmatization 2-grams bag-of-words\n",
      "Preprocessing with highest f1-score: lemmatization 2-grams bag-of-words\n",
      "\n",
      "Random Forest\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: stemming 1-grams bag-of-words\n",
      "Preprocessing with highest f1-score: stemming 1-grams bag-of-words\n",
      "\n",
      "Performing DL experiments...\n",
      "CNN\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n",
      "LSTM\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: pretrained GloVe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CDataSet('euvsdisinfo','./datasets/euvsdisinfo/euvsdisinfo.csv', './datasets/euvsdisinfo/',max_len=50)\n",
    "dataset.perform_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e42bd3b-2238-4274-baab-511b8c6e2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ML experiments...\n",
      "Naive Bayes\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: stemming 2-grams bag-of-words\n",
      "Preprocessing with highest f1-score: stemming 2-grams bag-of-words\n",
      "\n",
      "Random Forest\n",
      "\n",
      "stemming 1-grams bag-of-words\n",
      "stemming 2-grams bag-of-words\n",
      "stemming 3-grams bag-of-words\n",
      "stemming 1-grams tf–idf\n",
      "stemming 2-grams tf–idf\n",
      "stemming 3-grams tf–idf\n",
      "stemming Word2Vec\n",
      "lemmatization 1-grams bag-of-words\n",
      "lemmatization 2-grams bag-of-words\n",
      "lemmatization 3-grams bag-of-words\n",
      "lemmatization 1-grams tf–idf\n",
      "lemmatization 2-grams tf–idf\n",
      "lemmatization 3-grams tf–idf\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: stemming 2-grams tf–idf\n",
      "Preprocessing with highest f1-score: stemming 2-grams tf–idf\n",
      "\n",
      "Performing DL experiments...\n",
      "CNN\n",
      "\n",
      "Word2Vec\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A9C6457C40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "stemming Word2Vec\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001A9C8939BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: lemmatization Word2Vec\n",
      "Preprocessing with highest f1-score: lemmatization Word2Vec\n",
      "\n",
      "LSTM\n",
      "\n",
      "Word2Vec\n",
      "stemming Word2Vec\n",
      "lemmatization Word2Vec\n",
      "pretrained GloVe\n",
      "Preprocessing with highest accuracy: pretrained GloVe\n",
      "Preprocessing with highest f1-score: Word2Vec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CDataSet('cz_dataset','./datasets/cz_dataset/cz_dataset.csv', './datasets/cz_dataset/',language='cs')\n",
    "dataset.perform_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
